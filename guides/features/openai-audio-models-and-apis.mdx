---
title: "Building With OpenAI Audio Models and APIs"
sidebarTitle: "OpenAI Audio Models and APIs"
description: "Create voice agents with OpenAI audio models and Pipecat"
---

This guide provides an overview of the audio capabilities OpenAI offers via their APIs. We'll also link to Pipecat sample code.

## Two Ways To Build Voice-to-voice

You can build voice-to-voice applications in two ways:

1. The cascaded models approach, using separate models for transcription, the LLM, and voice generation.

<Frame>![OpenAI Cascaded Pipeline](/images/openai-cascade.jpg)</Frame>

A cascaded pipeline looks like this, in Pipecat code. Here's a [single-file example that uses a cascaded pipeline](https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/07g-interruptible-openai.py). (See below for an overview of Pipecat core concepts.)

```python
pipeline = Pipeline(
    [
        transport.input(),
        speech_to_text,
        context_aggregator.user(),
        llm,
        text_to_speech,
        context_aggregator.assistant(),
        transport.output(),
    ]
)
```

2. Using a single, speech-to-speech model. This is conceptually much simpler. Though note that most applications also need to implement things like function calling, retrieval-augmented search, context management, and integration with existing systems. So the core pipeline is only part of an app's complexity.

<Frame>![OpenAI S2S Pipeline](/images/openai-s2s.jpg)</Frame>

Here's a speech-to-speech pipeline in Pipecat code. And here's a [single-file example that uses the OpenAI Realtime API](https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/19-openai-realtime-beta.py).

```python
pipeline = Pipeline(
    [
        transport.input(),
        context_aggregator.user(),
        speech_to_speech_llm,
        context_aggregator.assistant(),
        transport.output(),
    ]
)
```

Which approach should you choose?

- The cascaded models approach is preferable if you are implementing a complex workflow and need the best possible instruction following performance and function calling reliability. The `gpt-4o` model operating in text-to-text mode has the strongest instruction following and function calling performance.
- The speech-to-speech approach offers better audio understanding and human-like voice output. If your application is primarily free-form, open-ended conversation, these attributes might be more important than instruction following and function calling performance. Note also that `gpt-4o-audio-preview` and the OpenAI Realtime API are currently beta products.

## OpenAI Audio Models and APIs

### Transcription API

- Models: `gpt-4o-transcribe`, `gpt-4o-mini-transcribe`
- Pipecat service: `OpenAISTTService` ([reference docs](/server/services/stt/openai))
- OpenAI endpoint: `/v1/audio/transcriptions` ([docs](https://platform.openai.com/docs/api-reference/audio/createTranscription))

### Chat Completions API

- Models: `gpt-4o`, `gpt-4o-mini`, `gpt-4o-audio-preview`
- Pipecat service: `OpenAILLMService` ([reference docs](/server/services/llm/openai))
- OpenAI endpoint: `/v1/chat/completions` ([docs](https://platform.openai.com/docs/api-reference/chat))

### Realtime API

- Models: `gpt-4o-realtime-preview`, `gpt-4o-mini-realtime-preview`
- Pipecat service: `OpenAIRealtimeBetaLLMService` ([reference docs](/server/services/s2s/openai))
- OpenAI docs ([overview](https://platform.openai.com/docs/guides/realtime))

### Speech API

- Models: `gpt-4o-mini-tts`
- Pipecat service: `OpenAITTSService` ([reference docs](/server/services/tts/openai))
- OpenAI endpoint: `/v1/audio/speech` ([docs](https://platform.openai.com/docs/api-reference/audio/createSpeech))

## Sample code and starter kits

_If you have a code example or starter kit you would like this doc to link to, please let us know. We can add examples that help people get started with the OpenAI audio models and APIs._

### Single-file examples

<CardGroup cols={2}>
  <Card
    title="OpenAI STT → LLM → TTS"
    icon="code"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/07g-interruptible-openai.py"
  >
    A complete implementation demonstrating the cascaded approach with OpenAI services
  </Card>

  <Card
    title="OpenAI Realtime API"
    icon="tower-broadcast"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/19-openai-realtime-beta.py"
  >
    A speech-to-speech implementation using OpenAI's Realtime API
  </Card>
</CardGroup>

### OpenAI + Twilio + Pipecat Cloud

[This starter kit](https://github.com/daily-co/pcc-openai-twilio/) is a complete telephone voice agent that can talk about the NCAA March Madness basketball tournaments and look up realtime game information using function calls.

<Frame>
![OpenAI Twilio](/images/openai-twilio.png)

</Frame>

The starter kit includes two bot configurations: cascaded model and speech-to-speech. The code can be packaged for deployment to Pipecat Cloud, a commercial platform for Pipecat agent hosting.
